{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd8d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import glob\n",
    "import shutil\n",
    "from random import randint\n",
    "from os import listdir\n",
    "from PIL import Image as PImage\n",
    "from skimage import color, io\n",
    "from numpy import asarray\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f601cda2",
   "metadata": {},
   "source": [
    "### Build stacked autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e52fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define the autoencoder model\n",
    "def SAE (X_train, X_test ):  \n",
    "    # This is our input image\n",
    "    input_dim = X_train[0].shape[0] \n",
    "\n",
    "    # encoder layers\n",
    "    input_layer =Input(shape=(input_dim, ), name='input')\n",
    "    encoder = Dense (128, activation='relu')(input_layer)\n",
    "    encoder = layers.Dense(64, activation='relu')(encoder)\n",
    "\n",
    "    # bottleneck layer\n",
    "    encoder = Dense (32, activation='relu', name='encoder_layer')(encoder)\n",
    "\n",
    "    # decoder layers\n",
    "    decoder = layers.Dense(64, activation='relu')(encoder)\n",
    "    decoder = layers.Dense(128, activation='relu')(decoder)\n",
    "\n",
    "    # Output Layer\n",
    "    decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "   # autoencoder.summary()\n",
    "\n",
    "    opt = optimizers.Adam(learning_rate = 0.001)\n",
    "    autoencoder.compile(optimizer=opt, loss='mse')   \n",
    "    history = autoencoder.fit(X_train, X_train, epochs=20,  batch_size = 50, validation_data=(X_test,X_test), verbose=0)\n",
    "    visualise (history)\n",
    "\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71599b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "def visualise(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a123f8",
   "metadata": {},
   "source": [
    "### Display reconstruction images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ced98a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def decoded_img(X_test, class_name, autoencoder):\n",
    "    X_test= shuffle(X_test)\n",
    "    decoded_img = autoencoder.predict(X_test, verbose=1)\n",
    "    \n",
    "    n=10\n",
    "    for i in range(n):\n",
    "        # Display original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(X_test[i].reshape(100, 100))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        \n",
    "        # Display reconstruction\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(decoded_img[i].reshape(100, 100))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        # save the decoded images\n",
    "        plt.savefig(class_name+str(i)+'.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213b6e88",
   "metadata": {},
   "source": [
    "### Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361bb4bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Extract_features(path, class_name,autoencoder):\n",
    "\n",
    "    # get the encoder layers\n",
    "    get_encoded_layers = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer(\"encoder_layer\").output)\n",
    "    features_vector=[]\n",
    "    \n",
    "    # Read your input image for which you need to extract features\n",
    "    for img in os.listdir(path): \n",
    "        image= cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE) \n",
    "        image=cv2.resize(image,(100,100))\n",
    "        image = np.reshape(image, np.prod(image.shape[:]))\n",
    "\n",
    "        encoded_imgs = get_encoded_layers.predict(image[None], verbose=0)[0]     \n",
    "        image= encoded_imgs.reshape(-1)     \n",
    "        features_vector.append(image)\n",
    "        \n",
    "    features_vector = np.array(features_vector)\n",
    "    print('Extract local features form class {} with shape {}'. format(class_name, features_vector.shape))\n",
    "\n",
    "    # Save your features as an numpy array    \n",
    "    np.save('features_'+class_name +'.npy', features_vector)\n",
    "    \n",
    "    return features_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15afc1d9",
   "metadata": {},
   "source": [
    "### PCA for dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2906712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_step(X,className):\n",
    "    sc = MinMaxScaler()\n",
    "    data_rescaled  = sc.fit_transform(X)\n",
    "\n",
    "    ### 95% of variance\n",
    "    pca = PCA(n_components = 0.95)\n",
    "    pca.fit(data_rescaled )\n",
    "    reduced = pca.transform(data_rescaled )\n",
    "    print('To get 95% of variance explained for class {} we need {} principal components.'. format(className, reduced.shape[1]))\n",
    "\n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65904fe8",
   "metadata": {},
   "source": [
    "### DBSCAN cluster algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1199188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN Parameter Estimation\n",
    "\n",
    "def DBSCAN_algorithm(X, class_Item):\n",
    "    #Step 1: Calculate the average distance between each point in the data set and its 20 nearest neighbors (my selected MinPts value).\n",
    "    neighbors = NearestNeighbors(n_neighbors=20)\n",
    "    neighbors_fit = neighbors.fit(X)\n",
    "    distances, indices = neighbors_fit.kneighbors(X)\n",
    "    \n",
    "    #Step 2: Sort distance values by ascending value and plot\n",
    "    # Plotting K-distance Graph\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:,1]\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(distances)\n",
    "    plt.title('K-distance Graph',fontsize=20)\n",
    "    plt.xlabel('Data Points sorted by distance',fontsize=14)\n",
    "    plt.ylabel('Epsilon',fontsize=14)\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute DBSCAN  \n",
    "    \n",
    "   # print('Input eps based on the elbow in the k-distance graph:')\n",
    "    eps=int(input('Input eps based on the elbow in the k-distance graph: '))\n",
    "    db = DBSCAN(eps=eps, min_samples=100,  metric='minkowski', algorithm='ball_tree', p=2).fit(X)   \n",
    "    \n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "    #The labels_ property contains the list of clusters and their respective points.\n",
    "    labels = db.labels_\n",
    "\n",
    "    # Number of clusters in labels\n",
    "    no_clusters_ = len(set(labels)) \n",
    "    \n",
    "    print('Estimated no. of clusters: %d' % no_clusters_)\n",
    "\n",
    "    #creating subfolders\n",
    "    Clusters =[]\n",
    "    save_path = 'E:/subclasses_dbscan/'\n",
    "    \n",
    "    for c in [i for i in range(no_clusters_)]:\n",
    "        Cluster_name = class_Item +'_'+ str(c)\n",
    "        path2 = os.path.join(save_path, Cluster_name)\n",
    "        os.mkdir(path2)\n",
    "        Clusters.append(Cluster_name)        \n",
    "        \n",
    "    # assign each image into the corresponding label\n",
    "    for img, j in zip(glob.iglob(os.path.join(src_dir,class_Item, '*.png')), db.labels_):\n",
    "        shutil.copy(img, save_path+'/'+ Clusters[j-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bdebbe",
   "metadata": {},
   "source": [
    "### K-means cluster algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f63f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_algorithm (X, class_Item, k_value ):\n",
    "    save_path = 'E:/subclasses_kmeans/'\n",
    "    \n",
    "    #Apply k_means cluster algorithm for the class \n",
    "    kmeans = KMeans(n_clusters=k_value, random_state=0).fit(X)\n",
    "    print('The number of samples is: ',Counter(kmeans.labels_)) #display the total number of samples in each cluster\n",
    "             \n",
    "    Clusters =[]\n",
    "    \n",
    "    #creating subfolders\n",
    "    for c in [i for i in range(k_value)]:\n",
    "        Cluster_name = class_Item +'_'+ str(c)\n",
    "        path2 = os.path.join(save_path, Cluster_name)\n",
    "        os.mkdir(path2)\n",
    "        Clusters.append(Cluster_name)        \n",
    "        \n",
    "    # assign each image into the corresponding label\n",
    "    for img, j in zip(glob.iglob(os.path.join(src_dir,class_Item, '*.png')), kmeans.labels_):\n",
    "        shutil.copy(img, save_path+'/'+ Clusters[j-1])\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9678214",
   "metadata": {},
   "source": [
    "### Load image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c749150",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src_dir= ('E:/TrainingDataset') #data path\n",
    "Folder_dir= os.listdir(src_dir)\n",
    "\n",
    "# determine parameter k; the number of classes in the class decomposition component.\n",
    "k=2\n",
    "\n",
    "for class_name in Folder_dir:\n",
    "    path = os.path.join(src_dir,class_name)    \n",
    "    image_list=[]\n",
    "\n",
    "    print('Read images in class:', class_name)\n",
    "    for img in os.listdir(path):\n",
    "        # load the dataset, resize it and collect it in one list\n",
    "        image= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE) \n",
    "        image=cv2.resize(image,(100,100))\n",
    "        image_list.append(image) \n",
    "\n",
    "\n",
    "    training_data=np.array(image_list)     #convert all data to array\n",
    "    training_data= shuffle(training_data)  #shuffle data before training\n",
    "    \n",
    "    # Divid the data set into two groups 80% training and 20% test sets\n",
    "    X_train,X_test= train_test_split(training_data, test_size =.20, shuffle  = True) \n",
    "    \n",
    "    # Clean and reshap the data as required by the model\n",
    "    X_train = X_train.astype('float32') / 255.\n",
    "    X_train = np.reshape(X_train, (len(X_train), np.prod(X_train.shape[1:])))\n",
    "    X_test = X_test.astype('float32') / 255.\n",
    "    X_test = np.reshape(X_test, (len(X_test), np.prod(X_train.shape[1:])))\n",
    "    \n",
    "    #check items in X_train and X_test   \n",
    "    print('x_train: ',X_train.shape)\n",
    "    print('x_test:  ',X_test.shape)\n",
    "    \n",
    "    # Run stacked autoencoder model(SAE)\n",
    "    print('Run stacked autoencoder model')\n",
    "    autoencoder = SAE(X_train, X_test)\n",
    "    \n",
    "    #visualize the reconstruction images\n",
    "    decoded_img(X_test, class_name, autoencoder)\n",
    "    \n",
    "    # Extract deep features from the images dataset\n",
    "    print('Extract features:')\n",
    "    features_vector= Extract_features(path, class_name,autoencoder)\n",
    "    \n",
    "    # apply PCA technique\n",
    "    print ('Apply PCA for dimensionality reduction')\n",
    "    pca_features= pca_step(features_vector,class_name)\n",
    "    \n",
    "    # Apply class decomposition approach with dbscan cluster algorithm\n",
    "    DBSCAN_algorithm(pca_features, class_name)\n",
    "    \n",
    "    # Apply class decomposition approach with k-means cluster algorithm\n",
    "    kmeans_algorithm(pca_features, class_name, k)\n",
    "    print('====================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83ccbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
