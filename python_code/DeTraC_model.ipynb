{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f02eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from skimage.io import imread, imshow\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, confusion_matrix\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import Input, Activation, Flatten, Dense \n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import load_img,ImageDataGenerator, array_to_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b41588f",
   "metadata": {},
   "source": [
    "### Loading images dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652aae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height=224\n",
    "img_width=224\n",
    "batch_size=50\n",
    "trainpath= ('E:/Dataset')\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2) # set validation split\n",
    "test_datagen = ImageDataGenerator(rescale=1 / 255.0)\n",
    "\n",
    "print(\"The data is being split into training and validation set\")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    trainpath,# This is the target directory\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset='training') # set as training data\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    trainpath, # same directory as training data\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=\"E:/testset/\",\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=1,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    seed=42)\n",
    "\n",
    "#define labels for training\n",
    "class_decomposition = train_generator.class_indices\n",
    "print('Number of classes after applying decomposition approach: ',class_decomposition)\n",
    "\n",
    "#define labels for testing\n",
    "org_classes = test_generator.class_indices\n",
    "print('original classes: ',org_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bd1586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the number of images in each class in the training dataset\n",
    "No_images_per_class = []\n",
    "Class_name = []\n",
    "\n",
    "for i in os.listdir('E:/Dataset'):\n",
    "    train_class = os.listdir(os.path.join('E:/Dataset', i))\n",
    "    No_images_per_class.append(len(train_class))\n",
    "    Class_name.append(i)\n",
    "    print('Number of images in {} = {} \\n'.format(i, len(train_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb3432",
   "metadata": {},
   "outputs": [],
   "source": [
    "Class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5fcc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the augmented data\n",
    "sample_x, sample_y = next(train_generator)\n",
    "plt.figure(figsize=(12, 9))\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sample = array_to_img(sample_x[i])\n",
    "    plt.axis('off')\n",
    "    plt.grid(False)\n",
    "    plt.imshow(sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c150820a",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "### Loading the ImageNet pre-trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11ddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model without the output of the last convolution block \n",
    "base_model = ResNet50(include_top=False, input_shape=(224, 224, 3), weights = 'imagenet')\n",
    "\n",
    "# Flatten the output layer to 1 dimension\n",
    "x = Flatten()(base_model.output)\n",
    "\n",
    "# Add a fully connected layer with 2048 hidden units and ReLU activation\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# Adapt the final classification layer of an ImageNet pre-trained cNN model to the decomposed classes.\n",
    "predictions = Dense(len(class_decomposition), activation='softmax')(x)\n",
    "\n",
    "base_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "base_model.summary()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79695f9",
   "metadata": {},
   "source": [
    "### Training the model based on deep-tuning mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe49e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define checkpoint\n",
    "checkpoint = ModelCheckpoint(filepath='DeTraC_model.hdf5', \n",
    "                             monitor='val_accuracy',\n",
    "                             save_best_only=True, \n",
    "                             mode='auto',\n",
    "                             verbose=1)   \n",
    "\n",
    "#early stopping\n",
    "earlystop=EarlyStopping( monitor=\"val_accuracy\", \n",
    "                        patience=10,  \n",
    "                        mode=\"auto\")\n",
    "\n",
    "model_callbacks = [checkpoint, earlystop]\n",
    "\n",
    "\n",
    "optimize = SGD(learning_rate=0.0001, decay=0.9 / 5, momentum=0.9, nesterov=True)\n",
    "DeTraC_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#compile model\n",
    "DeTraC_model.compile(loss='mse', optimizer=optimize, metrics=['accuracy']) \n",
    "    \n",
    "# train the model\n",
    "history=DeTraC_model.fit(train_generator, \n",
    "                               steps_per_epoch=train_generator.n//train_generator.batch_size,\n",
    "                               validation_data= validation_generator,\n",
    "                               validation_steps=validation_generator.n//validation_generator.batch_size,\n",
    "                               epochs=2, \n",
    "                               callbacks= model_callbacks, \n",
    "                               verbose=1, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8066fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a91ae",
   "metadata": {},
   "source": [
    "### Make a prediction on a test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efced28",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test , y_test = [] , []\n",
    "for i in range(test_generator.n//1):\n",
    "    a , b = test_generator.next()\n",
    "    x_test.extend(a) \n",
    "    y_test.extend(b)\n",
    "y_test= np.array(y_test)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307520b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the output\n",
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "test_generator.reset()\n",
    "\n",
    "#make prediction\n",
    "y_score=DeTraC_model.predict(test_generator,steps=STEP_SIZE_TEST,verbose=1) # array with the probability predictions made by the model\n",
    "y_prediction=np.argmax(y_score,axis=1)  ## predicted_class_indices\n",
    "#\n",
    "#\n",
    "# ground truth labels\n",
    "# array with the labels of the test data\n",
    "y_true = np.argmax(y_test, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa69e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbf6cf9",
   "metadata": {},
   "source": [
    "### Class composition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b81cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Refine the final classification using error-correction critira\n",
    "\n",
    "# determine parameter k; the number of classes in the class decomposition component.\n",
    "k=2\n",
    "\n",
    "correction_prediction=[]\n",
    "for i in y_prediction:\n",
    "      correction_prediction.append(i // k)\n",
    "    \n",
    "correction_prediction=np.array(correction_prediction)\n",
    "correction_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b1c977",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fd9c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get confusion matrix\n",
    "cm = confusion_matrix(y_true, correction_prediction)\n",
    "print(cm)\n",
    "\n",
    "#plot\n",
    "fig, ax = plot_confusion_matrix(conf_mat=cm,  figsize=(6, 6),\n",
    "                                colorbar=False,\n",
    "                                show_absolute=True,\n",
    "                                show_normed=False,\n",
    "                                class_names=org_classes,cmap=\"Blues\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#get classification report\n",
    "print(classification_report(y_true, correction_prediction, target_names=org_classes))\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "# compute sensitivity and specificity for multi classification \n",
    "res = []\n",
    "for l in list(range(len(org_classes))):\n",
    "    prec,recall,_,_ = precision_recall_fscore_support(np.array(y_true)==l,\n",
    "                                       np.array(correction_prediction)==l,\n",
    "                                              pos_label=True,average=None)\n",
    "    res.append([l,recall[0],recall[1]])  \n",
    "\n",
    "    df= pd.DataFrame(res,columns = ['class','sensitivity','specificity']) \n",
    "print(df)\n",
    "\n",
    "#overall sensitivity and specificity\n",
    "df[1:].mean(axis = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
